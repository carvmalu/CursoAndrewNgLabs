{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18520106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis (PCA) implementation\n",
    "\n",
    "    PCA finds the directions (principal components) of maximum variance\n",
    "    in the data and project the data onto these directions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=None):\n",
    "        \"\"\"\n",
    "        Initialize PCA\n",
    "\n",
    "        Args:\n",
    "            n_components: Number of principal components to keep\n",
    "            if None, keeps all components \n",
    "        \"\"\"\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.mean = None # mean of each feature (for centering)\n",
    "        self.components_ = None # principal components (eigenvectors)\n",
    "        self.explained_variance = None # variance explained by each component\n",
    "        self.explained_variance_ration_ = None # percentage of variance explained\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA model to the data\n",
    "\n",
    "        Steps:\n",
    "            1 - center the data (subtract mean)\n",
    "            2 - compute covariance matrix\n",
    "            3 - find eigenvectors and eigenvalues (principal components)\n",
    "            4 - sort components by explained variance\n",
    "            5 - select top n_components\n",
    "\n",
    "        Args:\n",
    "            X: input data, shape(n_samples, n_features)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # step 1, why? because PCA finds directions of variance. The mean doesn't affect variance,\n",
    "        # but centering ensures the first principal component goes through the origin\n",
    "        self.mean = np.mean(X, axis  = 0) # mean of each feature\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # step 2, why? because the covariance matrix captures relationships between features\n",
    "        # the (i,j) entry shows covariance between feature i and feature j\n",
    "        # we want to find directions that maximize covariance structure\n",
    "        covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "        # rowvar = false means each column is a variable (feature)\n",
    "        # Shape: (n_features, n_features)\n",
    "\n",
    "        # alternative: you could compute X_centered.T @ X_centered / (n_samples -1)\n",
    "        # this is mathematically equivalent but computationally different\n",
    "\n",
    "        # step 3, why? Because eigenvectors of covariance matrix  = principal components\n",
    "        # eigenvalues = variance explained by each component\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "        # eigh() is for symmetric matrices (covariance is symmetric)\n",
    "        # returns eigenvalues and eigenvectors sorted in ascending order\n",
    "\n",
    "        # step 4, why? we want components with highest variance first\n",
    "        idx = np.argsort(eigenvalues)[::-1] # indices for descending sort\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # step 5\n",
    "        self.components_ = eigenvectors\n",
    "        self.explained_variance = eigenvalues\n",
    "        self.explained_variance_ration_ = eigenvalues / eigenvalues.sum()\n",
    "\n",
    "        # if n_components specified, truncate \n",
    "        if self.n_components is not None:\n",
    "            self.components_ = self.components_[:, :self.n_components]\n",
    "            self.explained_variance = self.explained_variance[:, :self.n_components]\n",
    "            self.explained_variance_ration_ = self.explained_variance_ration_[:, :self.n_components]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to principal component space \n",
    "\n",
    "        Project data onto principal components: Y = X_centered * W\n",
    "        where W is matrix of principal components\n",
    "\n",
    "        Args:\n",
    "             X: input data, shape (n_samples, n_features)\n",
    "\n",
    "        returns:\n",
    "            X_transformed: Projected data, shape(n_samples, n_components)\n",
    "        \"\"\"\n",
    "\n",
    "        # center the data using stored mean\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Project onto principal components\n",
    "        # Each column of components_ is a principal component(eigenvector)\n",
    "        # Dot product projects each sample onto each component\n",
    "        X_transformed = X_centered @ self.components_\n",
    "\n",
    "        return X_transformed\n",
    "     \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA and transform data in one step\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"\n",
    "        Transform data back to original space\n",
    "        Reconstruct data from principal components: X_reconstructed = Y * W ^T + mean\n",
    "\n",
    "        Args:\n",
    "            X_transformed: Data in PCA space, shape (n_samples, n_components)\n",
    "\n",
    "        returns:\n",
    "            X_reconstructured: Data in original feature space\n",
    "        \"\"\"\n",
    "\n",
    "        # project back to original space: Y * w^t\n",
    "        X_reconstructed = X_transformed @ self.components_.T\n",
    "\n",
    "        # add back the mena\n",
    "        X_reconstructed = X_reconstructed + self.mean\n",
    "\n",
    "        return X_reconstructed\n",
    "    def get_cumulative_variance(self):\n",
    "        \"\"\"\n",
    "        Get cumulative explained variance ratio\n",
    "\n",
    "        Useful for deciding how many components to keep\n",
    "        \"\"\"\n",
    "        return np.cumsum(self.explained_variance_ration_)\n",
    "    \n",
    "    def visualize_pca():\n",
    "        \"\"\"\n",
    "        Create a simple 2D example to visualize PCA\n",
    "        \"\"\"\n",
    "        # Generate correlated 2D data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 200\n",
    "    \n",
    "        # Create data with strong correlation (elongated ellipsoid)\n",
    "        mean = [2, 3]\n",
    "        cov = [[2, 1.8], [1.8, 2]]  # Covariance matrix\n",
    "        X = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "    \n",
    "        # Fit PCA with 2 components (we'll use both for visualization)\n",
    "        pca = PCA(n_components=2)\n",
    "        X_transformed = pca.fit_transform(X)\n",
    "    \n",
    "        # Plot original data and principal components\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "        # Plot 1: Original data\n",
    "        axes[0].scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
    "        axes[0].scatter(pca.mean_[0], pca.mean_[1], color='red', s=100, label='Mean')\n",
    "    \n",
    "        # Draw principal components as arrows from the mean\n",
    "        scale = 3  # Scale factor for arrows\n",
    "        for i in range(2):\n",
    "            component = pca.components_[:, i]\n",
    "            axes[0].arrow(pca.mean_[0], pca.mean_[1], \n",
    "                     component[0] * scale * np.sqrt(pca.explained_variance_[i]),\n",
    "                     component[1] * scale * np.sqrt(pca.explained_variance_[i]),\n",
    "                     color='red', width=0.05, head_width=0.3, alpha=0.8,\n",
    "                     label=f'PC{i+1}' if i == 0 else \"\")\n",
    "    \n",
    "        axes[0].set_xlabel('Feature 1')\n",
    "        axes[0].set_ylabel('Feature 2')\n",
    "        axes[0].set_title('Original Data with Principal Components')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].axis('equal')\n",
    "    \n",
    "        # Plot 2: Transformed data (in PCA space)\n",
    "        axes[1].scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=0.6)\n",
    "        axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[1].set_xlabel('Principal Component 1')\n",
    "        axes[1].set_ylabel('Principal Component 2')\n",
    "        axes[1].set_title('Data in PCA Space (PC1 vs PC2)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].axis('equal')\n",
    "    \n",
    "        # Plot 3: Variance explained\n",
    "        axes[2].bar(range(1, 3), pca.explained_variance_ratio_ * 100, alpha=0.7)\n",
    "        axes[2].set_xlabel('Principal Component')\n",
    "        axes[2].set_ylabel('Variance Explained (%)')\n",
    "        axes[2].set_title('Variance Explained by Each Component')\n",
    "        axes[2].set_xticks([1, 2])\n",
    "        axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "        # Print statistics\n",
    "        print(\"=\"*60)\n",
    "        print(\"PCA RESULTS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Original data shape: {X.shape}\")\n",
    "        print(f\"Transformed data shape: {X_transformed.shape}\")\n",
    "        print(f\"\\nPrincipal Components (eigenvectors):\")\n",
    "        print(f\"PC1: {pca.components_[:, 0]}\")\n",
    "        print(f\"PC2: {pca.components_[:, 1]}\")\n",
    "        print(f\"\\nExplained Variance:\")\n",
    "        print(f\"PC1: {pca.explained_variance_[0]:.3f} ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "        print(f\"PC2: {pca.explained_variance_[1]:.3f} ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "        print(f\"\\nTotal variance in original data: {np.sum(np.var(X, axis=0)):.3f}\")\n",
    "        print(f\"Total variance captured by PCA: {np.sum(pca.explained_variance_):.3f}\")\n",
    "        \n",
    "        # Demonstrate reconstruction\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RECONSTRUCTION DEMONSTRATION:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Take one sample\n",
    "        sample_idx = 0\n",
    "        original_sample = X[sample_idx]\n",
    "        \n",
    "        # Transform to PCA space (1 component only for compression demo)\n",
    "        pca_1d = PCA(n_components=1)\n",
    "        X_1d = pca_1d.fit_transform(X)\n",
    "        \n",
    "        # Reconstruct from 1D representation\n",
    "        reconstructed_1d = pca_1d.inverse_transform(X_1d[sample_idx].reshape(1, -1))\n",
    "        \n",
    "        print(f\"Original sample: {original_sample}\")\n",
    "        print(f\"Transformed to 1D (PC1 value): {X_1d[sample_idx][0]:.3f}\")\n",
    "        print(f\"Reconstructed from 1D: {reconstructed_1d[0]}\")\n",
    "        print(f\"Reconstruction error (1 component): {np.linalg.norm(original_sample - reconstructed_1d[0]):.3f}\")\n",
    "        \n",
    "        # Compare with 2-component reconstruction (should be perfect)\n",
    "        reconstructed_2d = pca.inverse_transform(X_transformed[sample_idx].reshape(1, -1))\n",
    "        print(f\"\\nReconstruction error (2 components): {np.linalg.norm(original_sample - reconstructed_2d[0]):.3f}\")\n",
    "        print(\"(Should be ~0 since we kept all components)\")\n",
    "\n",
    "    # Run the visualization\n",
    "    visualize_pca()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
